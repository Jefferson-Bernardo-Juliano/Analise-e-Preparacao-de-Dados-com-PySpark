# ğŸ§  Projeto de AnÃ¡lise e PreparaÃ§Ã£o de Dados com PySpark

OlÃ¡! ğŸ‘‹ Meu nome Ã© Jefferson e este repositÃ³rio foi criado como parte do meu aprendizado no mundo da **AnÃ¡lise de Dados**, utilizando **PySpark** para manipulaÃ§Ã£o e transformaÃ§Ã£o de dados em grande escala.

O projeto estÃ¡ dividido em quatro etapas principais, cada uma representada por um notebook. Abaixo explico cada etapa de forma simples e direta.

---

## ğŸ“ Arquivos do Projeto

### 1. `preparacao.ipynb` â€“ **PreparaÃ§Ã£o dos Dados**
> ğŸ”¹ **Objetivo**: Ler os arquivos brutos e preparar o ambiente para anÃ¡lise.

- Leitura de arquivos CSV e Parquet.
- CriaÃ§Ã£o da sessÃ£o Spark.
- VisualizaÃ§Ã£o inicial dos dados.
- Ajustes bÃ¡sicos como renomear colunas e remover colunas desnecessÃ¡rias.

ğŸ› ï¸ Ã‰ o primeiro passo: organizar os dados e preparar o ambiente.

---

### 2. `tratamento.ipynb` â€“ **Limpeza e Tratamento dos Dados**
> ğŸ”¹ **Objetivo**: Tratar dados inconsistentes ou nulos.

- RemoÃ§Ã£o de colunas irrelevantes (`_c0`, por exemplo).
- VerificaÃ§Ã£o e remoÃ§Ã£o de valores nulos.
- ConversÃ£o de tipos de dados (por exemplo, transformar strings em datas).
- CriaÃ§Ã£o de colunas auxiliares como `Year`, `Month` e `Day`.

ğŸ§¹ Essa etapa Ã© fundamental para garantir a qualidade dos dados antes de qualquer anÃ¡lise.

---

### 3. `agregacao.ipynb` â€“ **AgregaÃ§Ãµes e AnÃ¡lises ExploratÃ³rias**
> ğŸ”¹ **Objetivo**: Realizar agrupamentos e anÃ¡lises iniciais.

- Agrupamento de dados por colunas como `Keyword`, `Category`, etc.
- CÃ¡lculo de mÃ©tricas como contagem, mÃ©dias e valores mÃ¡ximos.
- CriaÃ§Ã£o de colunas como `Interaction` (soma de curtidas + comentÃ¡rios).
- Uso de funÃ§Ãµes de janela (Window Functions) para rankeamento.

ğŸ“Š Aqui comeÃ§am as descobertas e os insights sobre os dados.

---

### 4. `otimizacao.ipynb` â€“ **OtimizaÃ§Ã£o e Performance**
> ğŸ”¹ **Objetivo**: Melhorar a performance dos processos no PySpark.

- Particionamento de dados para escrita otimizada.
- Uso de `.repartition()` e `.coalesce()` para controlar paralelismo.
- Salvamento em formato Parquet com compressÃ£o.

âš™ï¸ Essa parte ajuda a preparar os dados para uso em pipelines maiores ou ambientes de produÃ§Ã£o.

---

## ğŸš€ Tecnologias Utilizadas

- Python ğŸ  
- Apache Spark (PySpark) âš¡  
- Google Colab ğŸ’»  
- Formatos de dados: CSV, Parquet 
