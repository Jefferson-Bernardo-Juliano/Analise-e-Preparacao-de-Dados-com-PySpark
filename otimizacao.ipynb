{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev9tbkk2YNqC",
        "outputId": "1c65b15e-60ec-4d18-f191-ecab79a872c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "# Instalação do PySpark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importações\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "Zmi6NISgb78n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Montar o Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SSZt3Btb70v",
        "outputId": "31a9987f-8b3b-4393-d584-1c1102e5fde6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar a SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "CwMJux2hb7q3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Leitura Arquivo\n",
        "df_video = spark.read.parquet(\"/content/drive/MyDrive/Colab Notebooks/projeto/videos-comments-tratados-parquet/videos-preparados.snappy.parquet\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "y5i4EDnPcMUX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Leitura Arquivo\n",
        "df_comments = spark.read.parquet(\"/content/drive/MyDrive/Colab Notebooks/projeto/videos-comments-tratados-parquet/videos-comments-tratados.snappy.parquet\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "RRWfWGQ6cMLY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar tabelas temporárias para consultas SQL\n",
        "df_video.createOrReplaceTempView(\"videos\")\n",
        "df_comments.createOrReplaceTempView(\"comments\")"
      ],
      "metadata": {
        "id": "yNuzAC2tcMCw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazer o join usando spark.sql\n",
        "join_video_comments = spark.sql(\"\"\"\n",
        "    SELECT v.*, c.*\n",
        "    FROM videos v\n",
        "    INNER JOIN comments c\n",
        "    ON v.`Video ID` = c.`Video ID`\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "K78kvrEucL8H"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Repartition\n",
        "df_video_rep = df_video.repartition(10, \"Video ID\")\n",
        "df_comments_rep = df_comments.repartition(10, \"Video ID\")\n",
        "\n",
        "df_video_rep.createOrReplaceTempView(\"videos_rep\")\n",
        "df_comments_rep.createOrReplaceTempView(\"comments_rep\")\n",
        "\n",
        "join_video_comments_rep = spark.sql(\"\"\"\n",
        "    SELECT v.*, c.*\n",
        "    FROM videos_rep v\n",
        "    INNER JOIN comments_rep c\n",
        "    ON v.`Video ID` = c.`Video ID`\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "v4W1AZjvcLxw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coalesce\n",
        "df_video_coalesce = df_video.coalesce(2)\n",
        "df_comments_coalesce = df_comments.coalesce(2)\n",
        "\n",
        "df_video_coalesce.createOrReplaceTempView(\"videos_coa\")\n",
        "df_comments_coalesce.createOrReplaceTempView(\"comments_coa\")\n",
        "\n",
        "join_video_comments_coa = spark.sql(\"\"\"\n",
        "    SELECT v.*, c.*\n",
        "    FROM videos_coa v\n",
        "    INNER JOIN comments_coa c\n",
        "    ON v.`Video ID` = c.`Video ID`\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "3jC3AhJdetkD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilizar o explain para entender os planos de execução\n",
        "print(\"Plano de execução padrão:\")\n",
        "join_video_comments.explain()\n",
        "\n",
        "print(\"Plano de execução com repartition:\")\n",
        "join_video_comments_rep.explain()\n",
        "\n",
        "print(\"Plano de execução com coalesce:\")\n",
        "join_video_comments_coa.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXNPBcvLe03T",
        "outputId": "74a735ef-19aa-4619-a93e-1edc40289c38"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plano de execução padrão:\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- BroadcastHashJoin [Video ID#1], [Video ID#28], Inner, BuildLeft, false\n",
            "   :- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, false]),false), [plan_id=39]\n",
            "   :  +- Filter isnotnull(Video ID#1)\n",
            "   :     +- FileScan parquet [Title#0,Video ID#1,Published At#2,Keyword#3,Likes#4,Comments#5,Views#6,Interaction#7,Year#8,Month#9,Keyword Index#10,Features PCA#11,Features Normal#12,Features#13] Batched: true, DataFilters: [isnotnull(Video ID#1)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/drive/MyDrive/Colab Notebooks/projeto/videos-comments-tr..., PartitionFilters: [], PushedFilters: [IsNotNull(`Video ID`)], ReadSchema: struct<Title:string,Video ID:string,Published At:date,Keyword:string,Likes:int,Comments:int,Views...\n",
            "   +- Filter isnotnull(Video ID#28)\n",
            "      +- FileScan parquet [Video ID#28,Title#29,Published At#30,Keyword#31,Likes#32,Comments#33,Views#34,Interaction#35,Year#36,Comment#37,Sentiment#38,Likes Comment#39] Batched: true, DataFilters: [isnotnull(Video ID#28)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/drive/MyDrive/Colab Notebooks/projeto/videos-comments-tr..., PartitionFilters: [], PushedFilters: [IsNotNull(`Video ID`)], ReadSchema: struct<Video ID:string,Title:string,Published At:date,Keyword:string,Likes:int,Comments:int,Views...\n",
            "\n",
            "\n",
            "Plano de execução com repartition:\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- BroadcastHashJoin [Video ID#1], [Video ID#28], Inner, BuildLeft, false\n",
            "   :- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, false]),false), [plan_id=72]\n",
            "   :  +- Exchange hashpartitioning(Video ID#1, 10), REPARTITION_BY_NUM, [plan_id=69]\n",
            "   :     +- Filter isnotnull(Video ID#1)\n",
            "   :        +- FileScan parquet [Title#0,Video ID#1,Published At#2,Keyword#3,Likes#4,Comments#5,Views#6,Interaction#7,Year#8,Month#9,Keyword Index#10,Features PCA#11,Features Normal#12,Features#13] Batched: true, DataFilters: [isnotnull(Video ID#1)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/drive/MyDrive/Colab Notebooks/projeto/videos-comments-tr..., PartitionFilters: [], PushedFilters: [IsNotNull(`Video ID`)], ReadSchema: struct<Title:string,Video ID:string,Published At:date,Keyword:string,Likes:int,Comments:int,Views...\n",
            "   +- Exchange hashpartitioning(Video ID#28, 10), REPARTITION_BY_NUM, [plan_id=70]\n",
            "      +- Filter isnotnull(Video ID#28)\n",
            "         +- FileScan parquet [Video ID#28,Title#29,Published At#30,Keyword#31,Likes#32,Comments#33,Views#34,Interaction#35,Year#36,Comment#37,Sentiment#38,Likes Comment#39] Batched: true, DataFilters: [isnotnull(Video ID#28)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/drive/MyDrive/Colab Notebooks/projeto/videos-comments-tr..., PartitionFilters: [], PushedFilters: [IsNotNull(`Video ID`)], ReadSchema: struct<Video ID:string,Title:string,Published At:date,Keyword:string,Likes:int,Comments:int,Views...\n",
            "\n",
            "\n",
            "Plano de execução com coalesce:\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- BroadcastHashJoin [Video ID#1], [Video ID#28], Inner, BuildLeft, false\n",
            "   :- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, false]),false), [plan_id=105]\n",
            "   :  +- Coalesce 2\n",
            "   :     +- Filter isnotnull(Video ID#1)\n",
            "   :        +- FileScan parquet [Title#0,Video ID#1,Published At#2,Keyword#3,Likes#4,Comments#5,Views#6,Interaction#7,Year#8,Month#9,Keyword Index#10,Features PCA#11,Features Normal#12,Features#13] Batched: true, DataFilters: [isnotnull(Video ID#1)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/drive/MyDrive/Colab Notebooks/projeto/videos-comments-tr..., PartitionFilters: [], PushedFilters: [IsNotNull(`Video ID`)], ReadSchema: struct<Title:string,Video ID:string,Published At:date,Keyword:string,Likes:int,Comments:int,Views...\n",
            "   +- Coalesce 2\n",
            "      +- Filter isnotnull(Video ID#28)\n",
            "         +- FileScan parquet [Video ID#28,Title#29,Published At#30,Keyword#31,Likes#32,Comments#33,Views#34,Interaction#35,Year#36,Comment#37,Sentiment#38,Likes Comment#39] Batched: true, DataFilters: [isnotnull(Video ID#28)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/drive/MyDrive/Colab Notebooks/projeto/videos-comments-tr..., PartitionFilters: [], PushedFilters: [IsNotNull(`Video ID`)], ReadSchema: struct<Video ID:string,Title:string,Published At:date,Keyword:string,Likes:int,Comments:int,Views...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Refazer join de forma otimizada com filter\n",
        "# --------------------------\n",
        "# Aqui filtramos os dados necessários antes do join para reduzir volume de dados.\n",
        "# - Selecionamos apenas colunas necessárias.\n",
        "# - Aplicamos filtros para reduzir linhas.\n",
        "# - Reparticionamos por chave de join para melhorar o shuffle.\n",
        "\n",
        "# Selecionando e filtrando apenas dados relevantes\n",
        "df_video_opt = df_video \\\n",
        "    .select(\"Video ID\", \"Title\", \"Keyword\") \\\n",
        "    .filter(\"Keyword = 'Education'\") \\\n",
        "    .repartition(10, \"`Video ID`\")\n",
        "\n",
        "df_comments_opt = df_comments \\\n",
        "    .select(\"Comment\", \"Video ID\", \"Likes\", \"Sentiment\") \\\n",
        "    .filter(\"Likes > 10\") \\\n",
        "    .repartition(10, \"`Video ID`\")\n",
        "\n",
        "# Criar tabelas temporárias otimizadas\n",
        "df_video_opt.createOrReplaceTempView(\"videos_opt\")\n",
        "df_comments_opt.createOrReplaceTempView(\"comments_opt\")\n",
        "\n",
        "# Fazer join otimizado apenas com colunas relevantes\n",
        "join_video_comments_opt = spark.sql(\"\"\"\n",
        "    SELECT v.`Video ID`, v.Title, c.Comment, c.Likes, c.Sentiment\n",
        "    FROM videos_opt v\n",
        "    INNER JOIN comments_opt c\n",
        "    ON v.`Video ID` = c.`Video ID`\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "qTuxXW-IfPx-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar o resultado do join otimizado\n",
        "# Salvar como parquet com compressão snappy para eficiência de armazenamento e leitura\n",
        "join_video_comments_opt.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(\"join-videos-comments-otimizado.snappy.parquet\")"
      ],
      "metadata": {
        "id": "R5Mah7k0gFqJ"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}